{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/home/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "DEBUG:matplotlib.backends:backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "from util import *\n",
    "import ujson as json\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from random import shuffle\n",
    "from keras.utils import Sequence\n",
    "from keras.optimizers import adam\n",
    "from nn_util import *\n",
    "logging.basicConfig(level=\"DEBUG\")\n",
    "# nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = load_json(\"Data/train_AddToPlaylist_full.json\")\n",
    "normal = load_json(\"Data/validate_AddToPlaylist.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full),len(normal) \n",
    "print (len(normal [\"AddToPlaylist\"]))\n",
    "list(full.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (normal [\"AddToPlaylist\"][0][\"data\"][0][\"text\"].split())\n",
    "normal [\"AddToPlaylist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_counter = Counter()\n",
    "for file in get_file_array(dir_name=\"Data\",matcher=\"*.json\"):\n",
    "    get_all_word(fname=file,main_counter=main_counter)\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(main_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting statistics for all datasets combined.\n",
    "main_counter = Counter()\n",
    "for file in get_file_array(dir_name=\"Data\",matcher=\"*.json\"):\n",
    "    get_all_word(fname=file,main_counter=main_counter)\n",
    "plt.hist([np.log10(count) for ele,count in main_counter.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dict = util.load_pickle(fname='word_vec.pkl')\n",
    "fname = 'Data/train_GetWeather_full.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_set = set()\n",
    "def asd(fname):\n",
    "    curr_dict = load_json(fname=fname)\n",
    "    key_list = list(curr_dict.keys())\n",
    "    assert (len(key_list) == 1)\n",
    "    label = key_list[0]\n",
    "    for dat in curr_dict[label]:\n",
    "        curr_sample = dat[\"data\"]\n",
    "        for i in curr_sample:\n",
    "            if 'entity' in i:\n",
    "                ent_set.add(i['entity'])\n",
    "[asd(fname) for fname in get_file_array(\"Data\",\"*.json\")]\n",
    "out_dict = {}\n",
    "for idx,val in enumerate(list(ent_set)):\n",
    "    out_dict[val] = idx\n",
    "# util.save_pickle(fname='ent_map.pkl',dat=out_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\"AddToPlaylist\": 0, \"BookRestaurant\":1, \"GetWeather\":2, \"PlayMusic\":3, \"RateBook\":4, \"SearchCreativeWork\":5, \"SearchScreeningEvent\":6}\n",
    "# util.save_pickle(fname='class_map.pkl',dat=class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentvector(fname):\n",
    "    class_map, ent_map = util.load_pickle(fname='class_map.pkl'), util.load_pickle(fname='ent_map.pkl')\n",
    "    vector_dict = util.load_pickle(fname='word_vec.pkl')\n",
    "    master_arr = []\n",
    "    curr_dict = load_json(fname)\n",
    "    key_list = list(curr_dict.keys())\n",
    "    zero_arr = np.zeros((40,300))\n",
    "    zero_ent_arr = np.zeros(len(ent_map))\n",
    "    assert (len(key_list) == 1)\n",
    "    label = key_list[0]\n",
    "    label_idx = class_map[label]\n",
    "    logging.info('Using file: {}\\nLabel: {}\\nTotal Samples: {}\\n'.format(fname, label, len(curr_dict[label])))\n",
    "    for dat in curr_dict[label]:\n",
    "        curr_sample = dat[\"data\"]\n",
    "        words_vec,idx = zero_arr.copy(),0\n",
    "        ent_vec = zero_ent_arr.copy()\n",
    "        for sec in curr_sample:\n",
    "            if 'entity' in sec:\n",
    "                ent_vec[ent_map[sec['entity']]] += 1\n",
    "            for word in sec['text'].split():\n",
    "                if word in vector_dict:\n",
    "                    words_vec[idx,:] = vector_dict[word]\n",
    "                    idx += 1\n",
    "                    if idx > 40:\n",
    "                        logging.warning('Encountered case with more than 40 words')\n",
    "                        break\n",
    "        master_arr.append({'words_vec': words_vec, 'entity':ent_vec})\n",
    "    util.save_pickle(fname=fname[:-5]+'.pkl',dat={'data':master_arr, 'label':label_idx})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname='Data/train_GetWeather_full.json'\n",
    "[get_sentvector(fname) for fname in get_file_array(\"Data\",\"*.json\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import numpy as np\n",
    "# Import all the required layers\n",
    "from keras.layers import Dense, Conv1D, Dropout, Input, Reshape, GlobalMaxPool1D\n",
    "# Import the concatenate layer\n",
    "from keras.layers.merge import concatenate\n",
    "import keras\n",
    "\n",
    "MAX_WORDS = 40  # Number of words\n",
    "EXTRA_DIM = 39  # Extra numbers for each class\n",
    "\n",
    "\n",
    "def load_all_data():\n",
    "    words_vec_arr, ent_arr, lab_arr = [], [], []\n",
    "    # Get all data in centralized array\n",
    "    for f_name in util.get_file_array('Data', '*.pkl'):\n",
    "        curr_dict = util.load_pickle(f_name)\n",
    "        label = curr_dict['label']\n",
    "        for dat in curr_dict['data']:\n",
    "            words_vec_arr.append(dat['words_vec'])\n",
    "            ent_arr.append(dat['entity'])\n",
    "            lab_arr.append(label)\n",
    "    # Randomly shuffle the data\n",
    "    words_vec_arr, ent_arr, lab_arr = np.array(words_vec_arr), np.array(ent_arr), np.array(lab_arr)\n",
    "    shuffle_arr = np.arange(len(words_vec_arr))\n",
    "    np.random.shuffle(shuffle_arr)\n",
    "    words_vec_arr, ent_arr, lab_arr = words_vec_arr[shuffle_arr], ent_arr[shuffle_arr], lab_arr[shuffle_arr]\n",
    "    return words_vec_arr, ent_arr, lab_arr\n",
    "\n",
    "\n",
    "def cnn_network():\n",
    "    # Definng input sentence\n",
    "    input_sent = Input(shape=(MAX_WORDS, 300), name='Input_Sent')\n",
    "    # Defining input entity\n",
    "    ent_vec = Input(shape=(EXTRA_DIM, ), name='ent_vec')\n",
    "\n",
    "    # Defining convolutions\n",
    "    c3 = Conv1D(filters=80, kernel_size=3, padding='same', name='C3')(input_sent)\n",
    "    c4 = Conv1D(filters=80, kernel_size=4, padding='same', name='C4')(input_sent)\n",
    "    c5 = Conv1D(filters=80, kernel_size=5, padding='same', name='C5')(input_sent)\n",
    "    c6 = Conv1D(filters=80, kernel_size=6, padding='same', name='C6')(input_sent)\n",
    "    c10 = Conv1D(filters=40, kernel_size=10, padding='same', name='C10')(input_sent)\n",
    "    # Doing 1D max pooling\n",
    "    m3 = GlobalMaxPool1D()(c3)\n",
    "    m4 = GlobalMaxPool1D()(c4)\n",
    "    m5 = GlobalMaxPool1D()(c5)\n",
    "    m6 = GlobalMaxPool1D()(c6)\n",
    "    m10 = GlobalMaxPool1D()(c10)\n",
    "    # Doing concatenation\n",
    "    combined_vec = concatenate([m3, m4, m5, m6, m10, ent_vec], name='Combining_Layers')\n",
    "    # Dense layer\n",
    "    fcl = Dense(units=100)(combined_vec)\n",
    "    final_layer = Dense(units=7)(fcl)\n",
    "    model = keras.Model(inputs=(input_sent, ent_vec), output=final_layer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.combine_data('Data',0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_plots(file_name,out_name=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(file_name)\n",
    "    print(df.columns)\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    l1 = ax1.plot(df[\"Epoch\"], df[\"Accuracy\"], 'b-',label=\"Training Accuracy\")\n",
    "    l2 = ax1.plot(df[\"Epoch\"], df[\"Validation_Accuracy\"], 'b--',label=\"Validation Accuracy\")\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy', color='b')\n",
    "    ax1.tick_params('y', colors='b')\n",
    "    \n",
    "    # Make the y-axis label, ticks and tick labels match the line color.\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.tick_params('y', colors='b')\n",
    "    l3 = ax2.plot(df[\"Epoch\"], df[\"Loss\"], 'r-',label=\"Training Loss\")\n",
    "    l4 = ax2.plot(df[\"Epoch\"], df[\"Validation_Loss\"], 'r--',label=\"Validation Loss\")\n",
    "    ax2.set_ylabel('Loss', color='r')\n",
    "    ax2.tick_params('y', colors='r')\n",
    "\n",
    "    # Printing Legend\n",
    "    lns = l1+l2+l3+l4\n",
    "    labs = [curr_line.get_label() for curr_line in lns ]\n",
    "    ax1.legend(lns, labs, loc='center')\n",
    "    if out_name:\n",
    "        plt.savefig(out_name,dpi=300)\n",
    "        return\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('dark')\n",
    "generate_plots(\"run1.txt\",\"run1.png\")\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required pickles\n",
    "class_map = load_pickle(fname='Data/class_map.pkl')\n",
    "vector_dict = load_pickle(fname='Data/word_vec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_ent_vec(sentence):\n",
    "#     nlp_obj = nlp(sentence)\n",
    "#     return [ele.label_ for ele in nlp_obj.ents if hasattr(ele,'label_')]\n",
    "#     [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "\n",
    "\n",
    "def make_combined_data(dir_name, regex_exp):\n",
    "    master_dict = {}\n",
    "    f_arr = get_file_array(dir_name=dir_name,matcher=regex_exp)\n",
    "    logging.info('Total number of files: {}'.format(len(f_arr)))\n",
    "    for fname in f_arr:\n",
    "        curr_dict = load_json(fname)\n",
    "        key_list = list(curr_dict.keys())\n",
    "        assert (len(key_list) == 1)\n",
    "        label = key_list[0]\n",
    "        if label not in master_dict:\n",
    "            master_dict[label] = []\n",
    "        logging.info('Using file: {}\\nLabel: {}\\nTotal Samples: {}\\n'.format(fname, label, len(curr_dict[label])))\n",
    "        for dat in curr_dict[label]:\n",
    "            # Processing for each sample\n",
    "            curr_sample, words_arr, idx = dat[\"data\"],[], 0\n",
    "            # Processing each sample\n",
    "            for sec in curr_sample:\n",
    "                # Processing each word\n",
    "                for word in sec['text'].split():\n",
    "                    words_arr.append(word)\n",
    "                    idx += 1\n",
    "                    if idx > 40:\n",
    "                        logging.warning('Encountered case with more than 40 words')\n",
    "                        break\n",
    "            master_dict[label].append(words_arr)\n",
    "    save_pickle(dat=master_dict, fname='Data/combined_dat.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_combined_data(dir_name='Data',regex_exp='*.json')\n",
    "from random import shuffle\n",
    "a = [1,2,3,4,5]\n",
    "shuffle(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(fields,tr_split,val_split,hard_cutoff):\n",
    "    # Load the combined data\n",
    "    main_dat = load_pickle('Data/combined_dat.pkl')\n",
    "    logging.info('Classwise split is performed')\n",
    "    tr_dict, val_dict, test_dict = {},{},{}\n",
    "    if not hard_cutoff and (tr_split + val_split > 1):\n",
    "        logging.warning('Total split has to be less than 1 \\n Exiting...'),exit()\n",
    "    test_split = 1 - tr_split - val_split\n",
    "    for key in main_dat.keys():\n",
    "        if key not in fields:\n",
    "            continue\n",
    "        # Randomly shuffle the array\n",
    "        shuffle(main_dat[key])\n",
    "        # Get indices\n",
    "        if hard_cutoff:\n",
    "            val_st,val_end = tr_split,tr_split+val_split\n",
    "        else:\n",
    "            val_st = int(tr_split*len(main_dat[key]))\n",
    "            val_end = val_st + int(val_split*len(main_dat[key]))\n",
    "        logging.info('For field:{} Train:{} Validation:{} Test:{}'.format(\n",
    "            key, val_st, val_end-val_st, len(main_dat[key]) - val_end))\n",
    "        if len(main_dat[key]) - val_end <= 0:\n",
    "            print(len(main_dat[key]) - val_end)\n",
    "            logging.warning('No test samples for field:{} \\n Exiting...'.format(key))\n",
    "        # Split dataset and store it\n",
    "        tr_dict[key] = main_dat[key][:val_st]\n",
    "        val_dict[key] = main_dat[key][val_st:val_end]\n",
    "        test_dict[key] = main_dat[key][val_end:]\n",
    "    save_pickle(dat=tr_dict,fname='Data/tr.pkl'), save_pickle(dat=val_dict, fname='Data/val.pkl')\n",
    "    save_pickle(dat=test_dict, fname='Data/test.pkl')\n",
    "    logging.info('Saved data after the split (test, tr, val).pkl')\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loader(Sequence):\n",
    "    \n",
    "    def __init__(self, fname,word_vec_file,batch_size,max_words,num_class):\n",
    "        from keras.utils.np_utils import to_categorical\n",
    "        self.dat = load_pickle(fname)\n",
    "        self.class_map = load_pickle('Data/class_map.pkl')\n",
    "        self.word_vec = load_pickle(word_vec_file)\n",
    "        self.keys = list(self.dat.keys())\n",
    "        self.batch_size = batch_size\n",
    "        self.max_words = max_words\n",
    "        logging.info('Pickle file loaded: {}'.format(fname))\n",
    "        \n",
    "        # Populating array\n",
    "        self.dat_arr,self.lab_arr = [],[]\n",
    "        for label,val in self.dat.items():\n",
    "            self.dat_arr += self.dat[label]\n",
    "            self.lab_arr += [self.class_map[label]]*len(self.dat[label])\n",
    "        self.dat_arr= np.array(self.dat_arr)\n",
    "        self.lab_arr = [np.expand_dims(to_categorical(ele,num_class),0) for ele in self.lab_arr]\n",
    "\n",
    "        # Initialize array for input  data (BS*MAX_NUM_WORDS*300)\n",
    "        self.input_dat_batch = np.zeros((self.batch_size, self.max_words,300))\n",
    "        self.lab_arr = np.array(self.lab_arr)\n",
    "        self.shuffle_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dat[self.keys[0]])//self.batch_size\n",
    "\n",
    "    def shuffle_data(self):\n",
    "        idx_num = np.arange(len(self.dat_arr))\n",
    "        np.random.shuffle(idx_num)\n",
    "        self.dat_arr = self.dat_arr[idx_num]\n",
    "        self.lab_arr = self.lab_arr[idx_num]\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.shuffle_data()\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        # Create shallow copy of the input_dat\n",
    "        x = self.input_dat_batch.copy()\n",
    "        \n",
    "        def produce_vector():\n",
    "            for idx, text in enumerate(text_arr):\n",
    "                if idx >= self.max_words:\n",
    "                    return \n",
    "                if text in self.word_vec:\n",
    "                    x[offset,idx,:] = self.word_vec[text]\n",
    "                \n",
    "        offset = 0\n",
    "        for sent_idx in range(index*self.batch_size,(index+1)*self.batch_size):\n",
    "            text_arr = self.dat_arr[sent_idx]\n",
    "            produce_vector()\n",
    "            offset += 1\n",
    "        # Concatenate all the label arrays\n",
    "        y = np.concatenate(self.lab_arr[index*self.batch_size:(index+1)*self.batch_size])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pickle file loaded: Data/tr.pkl\n"
     ]
    }
   ],
   "source": [
    "a = loader(batch_size=32,fname='Data/tr.pkl',num_class=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.99 ms, sys: 1.61 ms, total: 3.6 ms\n",
      "Wall time: 10.1 ms\n",
      "CPU times: user 2.77 ms, sys: 1.49 ms, total: 4.26 ms\n",
      "Wall time: 3.58 ms\n",
      "CPU times: user 13 µs, sys: 30 µs, total: 43 µs\n",
      "Wall time: 47.9 µs\n"
     ]
    }
   ],
   "source": [
    "%time a.on_epoch_end()\n",
    "%time a.__getitem__(1)\n",
    "%time a.__len__()\n",
    "all_class = set(ele for ele in load_pickle('Data/class_map.pkl').keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_data(all_class,1000,200,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AddToPlaylist',\n",
       " 'BookRestaurant',\n",
       " 'GetWeather',\n",
       " 'PlayMusic',\n",
       " 'RateBook',\n",
       " 'SearchCreativeWork',\n",
       " 'SearchScreeningEvent'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/home/PycharmProjects/ee/nn_util.py:98: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"In..., outputs=Tensor(\"de...)`\n",
      "  model = keras.Model(inputs=input_sent, output=final_layer)\n",
      "INFO:root:Summary of CNN model: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input_Sent (InputLayer)         (None, 40, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "C3 (Conv1D)                     (None, 40, 80)       72080       Input_Sent[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "C4 (Conv1D)                     (None, 40, 80)       96080       Input_Sent[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "C5 (Conv1D)                     (None, 40, 80)       120080      Input_Sent[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "C6 (Conv1D)                     (None, 40, 80)       144080      Input_Sent[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "C10 (Conv1D)                    (None, 40, 40)       120040      Input_Sent[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 80)           0           C3[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 80)           0           C4[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 80)           0           C5[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 80)           0           C6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 40)           0           C10[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "Combining_Layers (Concatenate)  (None, 360)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          36100       Combining_Layers[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            707         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 589,167\n",
      "Trainable params: 589,167\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pickle file loaded: Data/test.pkl\n"
     ]
    }
   ],
   "source": [
    "nn = cnn_network()\n",
    "nn.compile(optimizer=adam(0.000001),loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "nn.load_weights('nn_models/9_0.9792.pkl')\n",
    "test_loader = loader(batch_size=32, fname='Data/test.pkl', num_class=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = nn.predict_generator(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6084 190 6080\n"
     ]
    }
   ],
   "source": [
    "print(len(test_loader.dat_arr), test_loader.__len__(),len(dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "num_labels=7\n",
    "pred_label = np.argmax(dat,axis=1)\n",
    "ac_label = test_loader.cat_labels[:len(pred_label)]\n",
    "# Getting accuracy \n",
    "acc = [0]*num_labels\n",
    "for idx in range(num_labels):\n",
    "    idx_arr = [ac_label == idx]\n",
    "    acc[idx] = 100*(ac_label[idx_arr] == pred_label[idx_arr]).astype(np.int0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-08-25 09:29:47'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "datetime.fromtimestamp(time()).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
